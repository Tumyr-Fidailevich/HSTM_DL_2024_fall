{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6241b07b-2e75-44eb-ba94-41b3d668f350",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU:  True\n",
      "None\n",
      "2.5.1+rocm6.2\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "print(\"GPU: \", torch.cuda.is_available())\n",
    "print(torch.cuda.empty_cache())\n",
    "print(torch.__version__)\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c7c8a984-377b-4c3f-ac90-94487016693f",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "                    transforms.PILToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2cf1233fadfc9052",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoded_pixels_to_masks(fname: str, df: pd.DataFrame):\n",
    "    fname_df = df[df['ImageId'] == fname]\n",
    "    masks = np.zeros((256 * 1600, 4), dtype=int) # float32 is V.Imp\n",
    "\n",
    "    for i_row, row in fname_df.iterrows():\n",
    "        cls_id = row['ClassId']\n",
    "        encoded_pixels = row['EncodedPixels']\n",
    "        if encoded_pixels is not np.nan:\n",
    "            pixel_list = list(map(int, encoded_pixels.split(' ')))\n",
    "            for i in range(0, len(pixel_list), 2):\n",
    "                start_pixel = pixel_list[i] - 1\n",
    "                num_pixel = pixel_list[i+1]\n",
    "                masks[start_pixel:(start_pixel+num_pixel), cls_id-1] = 1\n",
    "                \n",
    "    masks = masks.reshape(256, 1600, 4, order='F')\n",
    "\n",
    "    return masks\n",
    "\n",
    "def masks_to_encoded_pixels(masks: np.ndarray):\n",
    "    masks = masks.reshape(256*1600, 4, order='F')\n",
    "    encoded_pixels_list = []\n",
    "    for cls_id in range(4):\n",
    "        cls_mask = masks[:, cls_id]\n",
    "        cls_mask = cls_mask.reshape(256, 1600, order='F')\n",
    "        cls_mask = cls_mask.T.flatten()\n",
    "        prev_pixel = 0\n",
    "        prev_pixel_val = 0\n",
    "        encoded_pixels = []\n",
    "        for i, pixel_val in enumerate(cls_mask):\n",
    "            if pixel_val != prev_pixel_val:\n",
    "                if pixel_val == 1:\n",
    "                    start_pixel = i + 1\n",
    "                    encoded_pixels.append(start_pixel - prev_pixel)\n",
    "                else:\n",
    "                    num_pixel = i - prev_pixel\n",
    "                    encoded_pixels.append(num_pixel)\n",
    "                prev_pixel = i\n",
    "                prev_pixel_val = pixel_val\n",
    "        encoded_pixels_list.append(encoded_pixels)\n",
    "    return encoded_pixels_list # shape: 4x[]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c3c4cf-4643-486a-8878-0379c60c03d5",
   "metadata": {},
   "source": [
    "### Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "5a51d514-96b5-4e00-a098-8d820ffbdb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeverstalSteelDataset(Dataset):\n",
    "    def __init__(self, df, img_dir, transform=None):\n",
    "        self.df = df.reset_index(drop=True) \n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.df.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        fname = self.df.ImageId[idx]\n",
    "        img_path = os.path.join(self.img_dir, fname)\n",
    "        img = Image.open(img_path)\n",
    "        img = np.array(Image.open(img_path).convert('RGB')) \n",
    "        masks = encoded_pixels_to_masks(img_path, self.df)\n",
    "        \n",
    "        img = torch.tensor(img, dtype=torch.float32).permute(2, 0, 1)\n",
    "        masks = torch.tensor(masks, dtype=torch.float32).permute(2, 0, 1)\n",
    "        # if self.transform:\n",
    "        #     img = self.transform(img)\n",
    "        \n",
    "        return fname, img, masks\n",
    "    \n",
    "# collate function if needed\n",
    "def collate_fn(batch_items):\n",
    "    batched_fnames = [item[0] for item in batch_items]\n",
    "    batched_imgs = torch.stack([item[1] for item in batch_items])\n",
    "    batched_masks = torch.stack([item[2] for item in batch_items])\n",
    "    return batched_fnames, batched_imgs, batched_masks\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b6bc81826579aca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SegModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SegModel, self).__init__()\n",
    "        self.model = smp.Unet(classes=4)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# class SegModel(torch.nn.Module):\n",
    "#     def __init__(self):\n",
    "#         super(SegModel, self).__init__()\n",
    "#         self.model = smp.DeepLabV3Plus(\n",
    "#             encoder_name=\"mobilenet_v2\",  # Лёгкий бэкбон\n",
    "#             encoder_weights=\"imagenet\",  # Предобученные веса\n",
    "#             classes=4,                   # Количество классов\n",
    "#             activation=None              # Без активации\n",
    "#         )\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2b4dbb99-65a4-43b7-bfd0-44b449201197",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dice_score(preds, targets, smooth=1e-6):\n",
    "    preds = preds.reshape(-1)\n",
    "    targets = targets.reshape(-1)\n",
    "    \n",
    "    intersection = (preds * targets).sum()\n",
    "    dice = (2.0 * intersection + smooth) / (preds.sum() + targets.sum() + smooth)\n",
    "    return dice.item()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a871ea7c-e520-44aa-8765-d7be360d7b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(csv_path, img_folder_path, batch_size=4, test_split=0.1, val_split=0.2, frac=1):\n",
    "    df = pd.read_csv(csv_path).sample(frac=frac, random_state=42).reset_index(drop=True)\n",
    "    train_val_df, test_df = train_test_split(df, test_size=test_split, random_state=42)\n",
    "    train_df, val_df = train_test_split(train_val_df, test_size=val_split, random_state=42)\n",
    "    \n",
    "    # Создаем датасеты\n",
    "    train_dataset = SeverstalSteelDataset(train_df, img_folder_path, transform=transform)\n",
    "    val_dataset = SeverstalSteelDataset(val_df, img_folder_path, transform=transform)\n",
    "    test_dataset = SeverstalSteelDataset(test_df, img_folder_path, transform=transform)\n",
    "    \n",
    "    # Создаем загрузчики\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,  num_workers=4)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9886aee35f0139bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    model = SegModel() \n",
    "    return model, torch.nn.BCEWithLogitsLoss(), torch.optim.Adam(model.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "\n",
    "def train(model, criterion, optimizer, loader, device, epochs=10):\n",
    "    model.to(device)\n",
    "    model.train()  # Устанавливаем модель в режим обучения\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        batch_idx = 0\n",
    "        for fname, imgs, masks in loader:\n",
    "            batch_idx += 1\n",
    "            imgs = imgs.to(device)\n",
    "            masks = masks.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(imgs)\n",
    "\n",
    "            loss = criterion(outputs, masks)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            # Печатаем статистику по ходу обучения\n",
    "            if batch_idx % 10 == 0:  # Печатаем каждый 10-й батч\n",
    "                print(f'Epoch [{epoch+1}/{epochs}], Step [{batch_idx+1}/{len(loader)}], Loss: {loss.item():.4f}')\n",
    "\n",
    "        # Выводим средний loss за эпоху\n",
    "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {running_loss/len(loader):.4f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "def validate(model, val_loader, device):\n",
    "    model.eval()\n",
    "    dice_scores = []\n",
    "    with torch.no_grad():\n",
    "        for fnames, imgs, masks in val_loader:\n",
    "            imgs, masks = imgs.to(device), masks.to(device)\n",
    "            outputs = model(imgs)\n",
    "            preds = torch.sigmoid(outputs) > 0.5 \n",
    "            preds = preds.cpu().numpy()\n",
    "            masks = batch_masks.cpu().numpy()\n",
    "\n",
    "            for i in range(len(fnames)):\n",
    "                for cls in range(4):\n",
    "                    dice = dice_score(preds[i, cls_id], masks[i, cls_id])\n",
    "                    dice_scores.append(dice)\n",
    "\n",
    "    mean_dice = np.mean(dice_scores)\n",
    "    print(f\"Avg dice score: {mean_dice:.4f}\")\n",
    "    for cls_id in range(4):\n",
    "        cls_dice = np.mean([score for i, score in enumerate(dice_scores) if i % 4 == cls_id])\n",
    "        print(f\"Dice Score for Class {cls_id + 1}: {cls_dice:.4f}\")\n",
    "\n",
    "def evaluate(model, loader, device): \n",
    "    model.eval()\n",
    "    submission = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for fnames, imgs, masks in test_loader:\n",
    "            imgs = imgs.to(device)\n",
    "            outputs = model(imgs)\n",
    "            preds = torch.sigmoid(outputs) \n",
    "\n",
    "            preds = preds.cpu().numpy()\n",
    "            for i, fname in enumerate(fnames):\n",
    "                cur_submission = []\n",
    "                for cls in range(4):\n",
    "                    encoded_pixels = masks_to_encoded_pixels(preds[i, cls])\n",
    "                    cur_submission.append((fname, cls + 1, encoded_pixels))\n",
    "\n",
    "                submission.extend(cur_submission)\n",
    "    return pd.DataFrame(submission, columns=['ImageId', 'ClassId', 'EncodedPixels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4f0933c5eb321cff",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, criterion, optimizer = init_model()\n",
    "train_loader, val_loader, test_loader = load_data(\"../data/train.csv\", \"../data/train_images\", frac=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d273a8-61af-481c-ba55-79540a737f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Step [11/128], Loss: 0.6258\n",
      "Epoch [1/10], Step [21/128], Loss: 0.5707\n",
      "Epoch [1/10], Step [31/128], Loss: 0.5286\n",
      "Epoch [1/10], Step [41/128], Loss: 0.4897\n",
      "Epoch [1/10], Step [51/128], Loss: 0.4580\n",
      "Epoch [1/10], Step [61/128], Loss: 0.4294\n",
      "Epoch [1/10], Step [71/128], Loss: 0.4073\n",
      "Epoch [1/10], Step [81/128], Loss: 0.3862\n",
      "Epoch [1/10], Step [91/128], Loss: 0.3693\n",
      "Epoch [1/10], Step [101/128], Loss: 0.3544\n",
      "Epoch [1/10], Step [111/128], Loss: 0.3358\n",
      "Epoch [1/10], Step [121/128], Loss: 0.3193\n",
      "Epoch [1/10], Loss: 0.4439\n",
      "Epoch [2/10], Step [11/128], Loss: 0.2948\n",
      "Epoch [2/10], Step [21/128], Loss: 0.2822\n",
      "Epoch [2/10], Step [31/128], Loss: 0.2685\n",
      "Epoch [2/10], Step [41/128], Loss: 0.2563\n",
      "Epoch [2/10], Step [51/128], Loss: 0.2450\n",
      "Epoch [2/10], Step [61/128], Loss: 0.2341\n",
      "Epoch [2/10], Step [71/128], Loss: 0.2228\n",
      "Epoch [2/10], Step [81/128], Loss: 0.2137\n",
      "Epoch [2/10], Step [91/128], Loss: 0.2039\n",
      "Epoch [2/10], Step [101/128], Loss: 0.1956\n",
      "Epoch [2/10], Step [111/128], Loss: 0.1867\n",
      "Epoch [2/10], Step [121/128], Loss: 0.1787\n",
      "Epoch [2/10], Loss: 0.2328\n",
      "Epoch [3/10], Step [11/128], Loss: 0.1653\n",
      "Epoch [3/10], Step [21/128], Loss: 0.1579\n",
      "Epoch [3/10], Step [31/128], Loss: 0.1517\n",
      "Epoch [3/10], Step [41/128], Loss: 0.1456\n",
      "Epoch [3/10], Step [51/128], Loss: 0.1389\n",
      "Epoch [3/10], Step [61/128], Loss: 0.1336\n",
      "Epoch [3/10], Step [71/128], Loss: 0.1280\n",
      "Epoch [3/10], Step [81/128], Loss: 0.1228\n",
      "Epoch [3/10], Step [91/128], Loss: 0.1190\n",
      "Epoch [3/10], Step [101/128], Loss: 0.1137\n",
      "Epoch [3/10], Step [111/128], Loss: 0.1094\n",
      "Epoch [3/10], Step [121/128], Loss: 0.1055\n",
      "Epoch [3/10], Loss: 0.1332\n",
      "Epoch [4/10], Step [11/128], Loss: 0.0987\n",
      "Epoch [4/10], Step [21/128], Loss: 0.0951\n",
      "Epoch [4/10], Step [31/128], Loss: 0.0916\n",
      "Epoch [4/10], Step [41/128], Loss: 0.0886\n",
      "Epoch [4/10], Step [51/128], Loss: 0.0854\n",
      "Epoch [4/10], Step [61/128], Loss: 0.0828\n",
      "Epoch [4/10], Step [71/128], Loss: 0.0800\n",
      "Epoch [4/10], Step [81/128], Loss: 0.0774\n",
      "Epoch [4/10], Step [91/128], Loss: 0.0747\n",
      "Epoch [4/10], Step [101/128], Loss: 0.0726\n",
      "Epoch [4/10], Step [111/128], Loss: 0.0701\n",
      "Epoch [4/10], Step [121/128], Loss: 0.0682\n",
      "Epoch [4/10], Loss: 0.0824\n",
      "Epoch [5/10], Step [11/128], Loss: 0.0644\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "model = train(model, criterion, optimizer, train_loader, device)\n",
    "validate(model, test_loader, device)\n",
    "submission_df = evaluate(model, test_loader, device)\n",
    "submission_df.to_csv(\"my_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ad6e06-9f05-4381-9972-d0562b8fbae6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_course",
   "language": "python",
   "name": "dl_course"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
